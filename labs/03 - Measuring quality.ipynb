{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1b1f94b9-7b73-4748-9286-d786a69bdf71",
   "metadata": {},
   "source": [
    "# 1. Assessor and analyst work\n",
    "\n",
    "## 1.0. Rating and criteria\n",
    "\n",
    "Please [open this document](https://static.googleusercontent.com/media/guidelines.raterhub.com/en//searchqualityevaluatorguidelines.pdf)\n",
    "and study chapters 13.0-13.4. Your task will be to assess the organic answers of search engines given the same query.\n",
    "\n",
    "## 1.1. Explore the page\n",
    "\n",
    "For the following search engines:\n",
    "- https://duckduckgo.com/\n",
    "- https://www.bing.com/\n",
    "- https://ya.ru/\n",
    "- https://www.google.com/\n",
    "\n",
    "Perform the same query: \"**How to get from Kazan to Voronezh**\".\n",
    "\n",
    "Discuss with your TA the following:\n",
    "1. Which elements you may identify at SERP? Ads, snippets, blends from other sources, ...?\n",
    "2. Where are organic results? How many of them are there?\n",
    "\n",
    "## 1.2. Rate the results of the search engine\n",
    "\n",
    "If there are many of you in the group, assess all search engines, otherwise choose 1 or 2. There should be no less than 5 of your for each search engine. Use the scale from the handbook, use 0..4 numerical equivalents for `[FailsM, SM, MM, HM, FullyM]`. \n",
    "\n",
    "Compute:\n",
    "- average relevance and standard deviation for each SERP element.\n",
    "- [Fleiss kappa score](https://en.wikipedia.org/wiki/Fleiss%27_kappa#Worked_example) for your group. Use [this implementation](https://www.statsmodels.org/dev/generated/statsmodels.stats.inter_rater.fleiss_kappa.html).\n",
    "- [Kendall rank coefficient](https://en.wikipedia.org/wiki/Kendall_rank_correlation_coefficient) for some pairs in your group. Use [this implementation](https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.kendalltau.html).\n",
    "\n",
    "Discuss numerical results. Did you agree on the relevance? Did you agree on the rank? What is the difference?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3c3738c6-ed54-4113-b75a-cba812c5aade",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "# example input by users\n",
    "ranking_data = np.array([\n",
    "    [4, 4, 4, 3, 4, 2, 2, 1, 1, 0], # assessor 1 relevance\n",
    "    [...],\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32a88b0e-5ece-404c-9fc2-82c4d1bb3281",
   "metadata": {},
   "source": [
    "Averages ang standard deviations per item.\n",
    "\n",
    "1) Calculate the Average (Hint use mean(axis=0) cause we care about the columns)\n",
    "\n",
    "2) Calculate Sigma2: the Variance (mean((item - mean)**2 ))\n",
    "\n",
    "3) Calcualte Sigma: the square root of the Variance\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "59c53e3e-120f-45ec-94c1-f02d061f0177",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 0 relevance 3.80 ± 0.400\n",
      " 1 relevance 3.80 ± 0.400\n",
      " 2 relevance 4.00 ± 0.000\n",
      " 3 relevance 3.60 ± 0.490\n",
      " 4 relevance 3.40 ± 0.490\n",
      " 5 relevance 2.20 ± 0.400\n",
      " 6 relevance 1.80 ± 0.400\n",
      " 7 relevance 1.00 ± 0.000\n",
      " 8 relevance 1.00 ± 0.000\n",
      " 9 relevance 1.00 ± 1.095\n"
     ]
    }
   ],
   "source": [
    "# TODO your code here\n",
    "average_relevance = ...\n",
    "sigma2 = ...\n",
    "sigma = ...\n",
    "\n",
    "for i in range(ranking_data.shape[1]):\n",
    "    print(f\" {i} relevance {average_relevance[i]:.2f} ± {sigma[i]:.3f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70a88642-87aa-42c1-b913-83eb9b22d973",
   "metadata": {},
   "source": [
    "Fleiss kappa score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "06e63c2c-090d-4da1-b0b8-45ec2538e635",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install statsmodels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "6cb6cbb6-7768-42f0-aed3-215488d15bc2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Agreement matrix:\n",
      "[[0 0 0 1 4]\n",
      " [0 0 0 1 4]\n",
      " [0 0 0 0 5]\n",
      " [0 0 0 2 3]\n",
      " [0 0 0 3 2]\n",
      " [0 0 4 1 0]\n",
      " [0 1 4 0 0]\n",
      " [0 5 0 0 0]\n",
      " [0 5 0 0 0]\n",
      " [2 2 0 1 0]]\n",
      "Categories: [0 1 2 3 4]\n",
      "Kappa: 0.5156081808396124\n"
     ]
    }
   ],
   "source": [
    "from statsmodels.stats.inter_rater import aggregate_raters, fleiss_kappa\n",
    "\n",
    "# TODO your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bd6069e-e8a1-4483-b1f2-bf5f4e982283",
   "metadata": {},
   "source": [
    "Kendall tau score is pairwise. Compare one to another."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "6e7f5f22-74a4-47cd-848a-09c8e1abb39c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "KendalltauResult(correlation=0.8336550215650926, pvalue=0.003100607493269036)"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from scipy.stats import kendalltau\n",
    "\n",
    "# TODO your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9eed044a-9a49-432d-b7a6-7591087d0de7",
   "metadata": {},
   "source": [
    "# 2. Engineer work\n",
    "\n",
    "You will create a bucket of URLs which are relevant for the query **\"free cloud git\"**. Then you will automate the search procedure using https://serpapi.com/, or https://developers.google.com/custom-search/v1/overview, or whatever.\n",
    "\n",
    "Then you will compute MRR@10 and Precision@10.\n",
    "\n",
    "## 2.1. Build your bucket here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e11aa566-0ab3-48d7-959c-ab6cb875d7ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "rel_bucket = [\n",
    "    \"gitpod.io\",\n",
    "    \"github.com\",\n",
    "    \"bitbucket.org\",\n",
    "    \"source.cloud.google.com\",\n",
    "    \"gitlab.com\",\n",
    "    \"sourceforge.net\",\n",
    "    \"aws.amazon.com/codecommit/\",\n",
    "    \"launchpad.net\",\n",
    "]\n",
    "\n",
    "query = \"free git cloud\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d501aa5e-7051-4039-930b-b293c49721e1",
   "metadata": {},
   "source": [
    "## 2.2. Relevance Assessment\n",
    "\n",
    "The purpose of this section is to introduce a function, `is_rel`, which evaluates the relevance of a document based on its URL.\n",
    "\n",
    "### Function Overview\n",
    "\n",
    "**Name:** `is_rel(resp_url)`\n",
    "\n",
    "**Arguments:**\n",
    "- `resp_url`: A string representing the URL of the document we wish to assess for relevance.\n",
    "\n",
    "**Returns:**\n",
    "- A boolean value (`True` or `False`) indicating whether the document is considered relevant.\n",
    "\n",
    "### Code Explanation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "450a610f-c121-4781-8719-1dcfd4892625",
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_rel(resp_url):\n",
    "    pass\n",
    "    # Loop through each URL our known relevant bucket\n",
    "        # If the current URL is found within the provided resp_url\n",
    "           # The document is relevant\n",
    "    # If we've gone through the entire rel_bucket and found no matches\n",
    "    # The document is not relevant"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c3c2ea8-19c1-44b5-8fc2-fa4e23182dae",
   "metadata": {},
   "source": [
    "## 2.3. Automation\n",
    "\n",
    "This section introduces a procedure to fetch search results from a search engine using an automation tool (in this case, `serpapi`). This tool allows us to programmatically obtain search results based on a query.\n",
    "\n",
    "### Procedure Overview\n",
    "\n",
    "1. Define the `api_key` to authenticate with the service.\n",
    "2. Construct a URL endpoint that specifies the search query and other parameters.\n",
    "3. Fetch the search results.\n",
    "4. Parse and display the results, while also assessing their relevance.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "434e58a5-77a8-4d54-a495-8540361f6df7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 6 places to host your git repository\n",
      "https://opensource.com/article/18/8/github-alternatives\n",
      "False\n",
      "\n",
      "2 GitHub: Let's build from here · GitHub\n",
      "https://github.com/\n",
      "True\n",
      "\n",
      "3 Gitpod: Always ready-to-code.\n",
      "https://www.gitpod.io/\n",
      "True\n",
      "\n",
      "4 GitLab: The DevSecOps Platform\n",
      "https://about.gitlab.com/\n",
      "True\n",
      "\n",
      "5 14 Git Hosting Services Compared | Tower Blog\n",
      "https://www.git-tower.com/blog/git-hosting-services-compared/\n",
      "False\n",
      "\n",
      "6 Bitbucket | Git solution for teams using Jira\n",
      "https://bitbucket.org/product\n",
      "True\n",
      "\n",
      "7 Best 13 Free Version Control Hosting Software Picks in 2023\n",
      "https://www.g2.com/categories/version-control-hosting/free\n",
      "False\n",
      "\n",
      "8 Git\n",
      "https://git-scm.com/\n",
      "False\n",
      "\n",
      "9 15 Best GitHub Alternatives (Free & Paid) in 2023\n",
      "https://www.guru99.com/github-alternative.html\n",
      "False\n",
      "\n",
      "10 Best free git hosting? : r/git\n",
      "https://www.reddit.com/r/git/comments/46t07s/best_free_git_hosting/\n",
      "False\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Import the requests module to send HTTP requests.\n",
    "import requests\n",
    "\n",
    "# The unique API key to access the serpapi service.\n",
    "api_key = \"5aff1ae53da3a991a97d770bf1991833ba30a97d68925ede4cb0003285c727ba\"\n",
    "\n",
    "# Construct the URL for fetching search results, specifying the query, language (English),\n",
    "# geographic location (US), and the domain (google.com).\n",
    "url = ...\n",
    "\n",
    "# Send a GET request to the constructed URL and parse the JSON response.\n",
    "js = requests.get(url).json()\n",
    "\n",
    "# Initialize a list to store relevance indicators (1 for relevant, 0 for non-relevant).\n",
    "rels = []\n",
    "\n",
    "# Iterate over the 'organic_results' section of the response.\n",
    "for result in js[\"organic_results\"]:\n",
    "    # Display the position and title of the search result.\n",
    "    # Display the link (URL) of the search result.\n",
    "    # Check the relevance of the link using the is_rel function and display the result.\n",
    "    # Append the relevance indicator (1 or 0) to the rels list.\n",
    "    print()  # Print a blank line for better visual separation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "70849db7-918b-4b17-b045-3a5a06927936",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 1, 1, 1, 0, 1, 0, 0, 0, 0]\n"
     ]
    }
   ],
   "source": [
    "# Display the final list of relevance indicators for all the search results.\n",
    "print(rels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "285eb724-2188-45d3-be78-cffb91bacee8",
   "metadata": {},
   "source": [
    "## 2.4. MRR (Mean Reciprocal Rank)\n",
    "\n",
    "MRR stands for \"Mean Reciprocal Rank\". It is a statistical measure used to evaluate the quality of a list of ranked items, specifically in information retrieval systems like search engines. MRR calculates the average of the reciprocal ranks of the first relevant item in the list.\n",
    "\n",
    "### Concept:\n",
    "\n",
    "If we have a set of ranked items, the reciprocal rank is the multiplicative inverse of the rank of the first relevant item. For example, if the first relevant item was in the 2nd position, its reciprocal rank is $\\frac{1}{2}$. If no relevant items are found, then a default rank (often the length of the list plus one) is used.\n",
    "\n",
    "The MRR is the average of the reciprocal ranks for a set of queries or lists.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "33c40e3b-24d2-4c7b-860e-d543c1281f01",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mrr(list_of_lists, k=10):\n",
    "    # Initialize a variable to accumulate the sum of reciprocal ranks.\n",
    "    r = ...\n",
    "    # Iterate over each list in the list of lists.\n",
    "        # If there's no relevant item in the list, use the default rank (k+1). \n",
    "        # Otherwise, compute the reciprocal rank of the first relevant item.\n",
    "    # Return the mean of the accumulated reciprocal ranks.\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e64c4e28-9ae7-4715-96a2-adfa13c5b00b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mrr([rels]) # BTW, why do I wrap the list into additional brackets? :)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d644d32d-3d15-4d81-b3b5-396af0e85dd1",
   "metadata": {},
   "source": [
    "## 2.5. Precision\n",
    "\n",
    "Precision is one of the fundamental metrics in information retrieval. It quantifies how many of the retrieved items (or documents) are relevant. Specifically, precision is defined as the ratio of relevant retrieved items to the total number of retrieved items. \n",
    "\n",
    "Mathematically:\n",
    "$$ \\text{Precision} = \\frac{\\text{Number of Relevant Retrieved Items}}{\\text{Total Number of Retrieved Items}} $$\n",
    "\n",
    "\n",
    "In the context of multiple sets of retrieved items (like multiple search queries), the mean precision is often calculated to provide an average measure across all the sets.\n",
    "\n",
    "### Code Explanation:\n",
    "## 2.5. Precision\n",
    "Compute mean precision:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "5be58a4a-deeb-48cd-9f81-cae0be9a8493",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mp(list_of_lists, k=10):\n",
    "    # Initialize a variable to accumulate the sum of precisions.\n",
    "    p = 0\n",
    "    # Iterate over each list in the list of lists.\n",
    "        # Calculate precision for the current list.\n",
    "        # sum(l) gives the number of relevant items in the list, and k is the total number of items in the list.\n",
    "    # Return the mean of the accumulated precisions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "1d3fbb74-9b3d-46a2-98d9-4a8894c8e8b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.4\n"
     ]
    }
   ],
   "source": [
    "print(mp([rels]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
